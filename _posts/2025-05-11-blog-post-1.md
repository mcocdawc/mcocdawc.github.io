---
title: 'Multithreading in python for numerical applications'
date: 2025-05-11
permalink: /posts/2025/05/blog-post-1
tags:
  - Parallelism
  - Python
  - GIL
  - Multithreading
  - Numerics
  - Numba
  - Scipy
---

This post is about how to achieve multi-threaded parallelism in python
specifically focused on numerical applications.

Why?
======

The first question to ask is **why do we want to apply multi-threading and when**?
The big advantage of threads over processes is that they use shared memory, which makes it
easier to write performant code, **if** memory is a bottleneck.
The big disadvantage of threads over processes is that they use shared memory,
which makes it easier to shoot yourself in the foot and to introduce race conditions and other complications into your program.

In the context of linear algebra applications and scientific programming one can
easily run into problems, where performance matters and where shared memory parallelism
can help to optimise a program.
I am assuming that we are actually in such a case and that we want to do this in python.

In the rest of this post, I will first discuss the "Global Interpreter Lock" (GIL)
which is often brought up asn a end-of-discussion argument as to why multi-threading is not possible in Python.
We will see, that for many problems the GIL is no obstacle and one can still use
multi-threaded code in native python.

We then construct a non-trivial example, where the GIL would be a problem,
but we will see that the [numpy](https://numpy.org/doc/2.2/index.html)/[scipy](https://scipy.org/) stack
and in particular [numba](https://numba.pydata.org/) makes it very easy to leverage multi-threaded parallelism also in this case.


The Global Interpreter Lock (GIL)
======

The GIL is for example described [here](https://wiki.python.org/moin/GlobalInterpreterLock)

> In CPython, the global interpreter lock, or GIL, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. [...]

This means that multi-threaded Python programs do not achieve true parallelism for CPU-bound tasks,
because only one thread is actually executed at time.
The crucial caveat is here **CPU-bound** tasks.
If, for example the CPU is just waiting for data to arrive from memory/disk/network,
i.e. if the task is I/O bound, then one can actually benefit from
parallelism even for native python code.

We can see this in the following example, where we define a `double` function
that takes artificially long to compute the double of the input.
```python
from time import sleep

def double(n):
    sleep(1)
    return 2 * n

double(1)   # takes 1 second
```

If we now compute the double of the numbers 0 to 9, we need 10 seconds in a serial implementation.
```python
[double(i) for i in range(10)]    # takes 10 seconds
```
So far, the timings are not surprising.

Since the function is pure we can trivially parallelize it across 10 threads
```python
with ThreadPoolExecutor(max_workers=10) as executor:
    submitted = [executor.submit(double, i) for i in range(10)]
    new_values = [x.result() for x in submitted]
```
This code **takes 1 second to complete**.
The reason is simply that the CPU does not do anything while sleeping,
hence it can execute the code of other threads, where it will be sleeping as well.
This means that all threads can "execute", namely sleep, in parallel and finish together after 1 second.

Of course this example is an extreme, but in many cases the CPU will idle and wait for data to arrive
in I/O-bound problems.
These problems can actually benefit from multi-threading already in native python.

Numpy/Scipy/Numba and the GIL
======

Native extension libraries written in C or C++ can explicitly release the GIL during time-consuming operations.

NumPy, like many C-extension libraries in Python, releases the GIL when performing computationally intensive operations in native code.
This means that while NumPy executes heavy numerical tasks, such as large matrix operations or array manipulations, it can temporarily free the GIL, allowing other Python threads to run concurrently.

In addition, if one uses numba to just-in-time-compile a (jitted) function it is also possible
to release the GIL for this particular function.

This makes it possible to take advantage of multi-threading for CPU-bound tasks.


Finding a CPU-bound example
======

In order to define a CPU-bound problem we need to keep the data small enough, so that it fits into the cache
and the CPU does not have to wait for memory to arrive.

A simple problem for this is the [Power Method](https://en.wikipedia.org/wiki/Power_iteration) to find
the highest/lowest eigenvalue of a matrix.
Practically speaking, we repeatedly apply a matrix to a vector until the vector does not change its direction anymore.
If the matrix and vector are small enough to fit into cache we are actually bound by the CPU.

In the following we define it as:
```python
import numba
import numpy as np
from numba import njit, prange
from numpy import sqrt
from numpy.linalg import norm

@njit(nogil=True, parallel=True)
def get_hermitian_matrices(n_matrices, matrix_size, rng):
    A = rng.uniform(low=-1., high=1., size=(n_matrices, matrix_size, matrix_size))
    for i in prange(n_matrices):
        A[i, :, :] = (A[i, :, :] + A[i, :, :].T) / 2
    return A

@njit(nogil=True)
def normalize(x):
    return x / norm(x)

@njit(nogil=True)
def next_guess(M, x):
    x_next = normalize(M @ x)
    return x_next, norm(x - x_next)

def _power_method(M, epsilon=1e-10, start_guess=None, max_iter=1e4):
    if start_guess is None:
        start_guess = np.zeros(len(M))
        start_guess[0] = 1

    x_next, error = next_guess(M, start_guess)
    iter = 1
    while error >= epsilon and iter <= max_iter:
        x_next, error = next_guess(M, x_next)
        iter += 1
    return ((M @ x_next) / x_next).mean()

power_method = njit(nogil=True)(_power_method)   # explicitly release the GIL
gil_power_method = njit(nogil=False)(_power_method)
```

This gives use a jitted function `power_method`, which releases the GIL,
and a jitted function `gil_power_method`, which does not release it.


Different parallelisations
======

We can now compute the highest eigenvalue from several matrices.
In principle, this is trivially parallelized.
We will have a serial implementation,
a parallel implementation that uses numba's `prange`, and
a parallel implementation that uses python's native `ThreadPoolExecutor`.

```python
@njit
def serial_get_eigenvalues(h_matrices):
    L = h_matrices.shape[0]
    lambdas = np.empty(L, dtype=np.float64)
    for i in range(L):
        lambdas[i] = power_method(h_matrices[i, :, :])
    return lambdas


@njit(parallel=True)
def parallel_get_eigenvalues(h_matrices):
    L = h_matrices.shape[0]
    lambdas = np.empty(L, dtype=np.float64)
    for i in prange(L):
        lambdas[i] = power_method(h_matrices[i, :, :])
    return lambdas

def parallel_python_code(h_matrices, power_method, n_threads=10):
    L = h_matrices.shape[0]

    with ThreadPoolExecutor(max_workers=n_threads) as executor:
        lambdas = [executor.submit(power_method, h_matrices[i, :, :]) for i in range(L)]
        lambdas = [x.result() for x in lambdas]
    return lambdas
```

If we time the results via
```python
n_threads = 10
%time serial_get_eigenvalues(h_matrices)
numba.set_num_threads(n_threads)
%time parallel_get_eigenvalues(h_matrices):
%time parallel_python_code(h_matrices, power_method, n_threads=n_threads)
%time parallel_python_code(h_matrices, gil_power_method, n_threads=n_threads)
```
we see that  the python code with `ThreadPoolExecutor` performs as fast as the numba jitted parallel version,
iff the gil is released; otherwise it is as fast as the serial version.